,id,title,authorids,authors,TL;DR,abstract,pdf,software,preprint,existing_preprints,preferred_venue,consent,paperhash,reviewer/Editor_reassignment_request,reviewer/Editor_reassignment_justification,data,previous_URL,previous_PDF,response_PDF,Abstract,author,forum
0,F8S005CJIsf,Heterogeneous-Graph Reasoning and Fine-Grained Aggregation for Fact Checking,,,,,/pdf?id=F8S005CJIsf,,,,,,,,,,,,,"Fact checking is a challenging task that requires corresponding evidences to verify the property of a claim based on reasoning. Previous studies generally i) construct the graph by treating each evidence-claim pair as node which is a simple way that ignores to exploit their implicit interaction, or building a fully-connected graph among claim and evidences where the entailment relationship between claim and evidence would be considered equally to the semantic relationship among evidences; ii) aggregate evidences equally without considering their different stances towards the verification of fact. Towards the above issues, we propose a novel heterogeneous-graph reasoning and fine-grained aggregation model, with two following modules: 1) a heterogeneous graph attention network module to distinguish different types of relationships within the constructed graph; 2) fine-grained aggregation module which learns the implicit stance of evidences towards the prediction result in details. Extensive experiments on the benchmark dataset demonstrate that our proposed model achieves much better performance than state-of-the-art methods.",Anonymous,/forum?id=F8S005CJIsf
1,U_cqFnIXla,A Character-level Ngram-based MT Approach for Lexical Normalization in Social Media,,,,,/pdf?id=U_cqFnIXla,,,,,,,,,,,,,"This paper presents an ngram-based MT approach that operates at character-level to generate possible canonical forms for lexical variants in social media text. It utilizes a joint n-gram model to learn edit sequences of word pairs, thus overcomes the shortage of phrase-based approach that is unable to capture dependencies across phrases. We evaluate our approach on two English tweet datasets and observe that the ngram-based approach significantly outperforms phrase-based approach in normalization task. Our simple model achieves a broad coverage on diverse variants which is comparable to complex hybrid systems.",Anonymous,/forum?id=U_cqFnIXla
2,15ZiKr2oSkC,Deep Reinforcement Learning-based Authentic Dialogue Generation To Protect Youth From Cybergrooming,,,,,/pdf?id=15ZiKr2oSkC,,,,,,,,,,,,,"Cybergrooming is defined as a crime towards potential victims, especially teens, by building close personal relationships with them with the purpose of sexual exploitation via online media. Cyber or online sexual grooming has been recognized as a serious cyber crime. However, there have been insufficient programs to proactively protect the youth from cybergrooming. In this work, we present a generative chatbot framework, called SERI (Stop cybERgroomIng), that can generate simulated conversations between a perpetrator chatbot and a potential victim chatbot. To realize the simulation of authentic conversations in the context of cybergrooming, we take deep reinforcement learning (DRL)-based dialogue generation for authentic simulation of the conversations between a potential victim and a perpetrator (i.e., cybergroomer). The design of the SERI is motivated to ensure a safe and authentic environment to strengthen the youth's precautionary awareness of cybergrooming while any unnecessary ethical issues (e.g., the potential misuse of the SERI) are removed or minimized. We developed the SERI as a preliminary platform that can deploy the perpetrator chatbot to interact with human users (i.e., youth) to observe youth users' responses to strangers or acquaintances and collect the reactions when the youth users are asked for private or sensitive information by the perpetrator. We evaluated the quality of conversations generated by the SERI based on open-source, referenced, unreferenced metrics, and human evaluation.",Anonymous,/forum?id=15ZiKr2oSkC
3,o6Cx94hha-b,Hybrid-Regressive Neural Machine Translation,,,,,/pdf?id=o6Cx94hha-b,,,,,,,,,,,,,"Non-autoregressive translation (NAT) with iterative refinement mechanism has shown comparable performance with the auto-regressive counterpart. However, we have empirically found that decoding acceleration is fragile when using a large batch size and running on the CPU. We demonstrate that one-pass NAT is sufficient when providing a few target contexts in advance through synthetic experiments. Inspired by this, we propose a two-stage translation prototype -- Hybrid-Regressive Translation (HRT) to combine the strengths of autoregressive and non-autoregressive. Specifically, HRT first generates a discontinuous sequence by autoregression (e.g., make a prediction every k tokens, k>1) and then fills all previously skipped tokens at once in a non-autoregressive manner. We also propose a bag of techniques to effectively and efficiently train HRT, with almost no increase in parameters. Experimental results on WMT En-Ro, En-De, and NIST Zh-En show that our model outperforms existing semi-autoregressive models and is competitive with current state-of-the-art non-autoregressive models. Moreover, compared to its autoregressive counterpart, HRT has a stable 1.5x acceleration, regardless of batch size and device.",Anonymous,/forum?id=o6Cx94hha-b
4,nWNq_AarEVd,When is BERT Multilingual? Isolating Crucial Ingredients for Cross-lingual Transfer,,,,,/pdf?id=nWNq_AarEVd,,,,,,,,,,,,,"While recent work on multilingual language models has demonstrated their capacity for cross-lingual zero-shot transfer on downstream tasks, there is a lack of consensus in the community as to what shared properties between languages enable such transfer. Analyses involving pairs of natural languages are often inconclusive and contradictory since languages simultaneously differ in many linguistic aspects. In this paper, we perform a large-scale empirical study to isolate the effects of various linguistic properties by measuring zero-shot transfer between four diverse natural languages and their counterparts constructed by modifying aspects such as the script, word order, and syntax. Among other things, our experiments show that the absence of sub-word overlap significantly affects zero-shot transfer when languages differ in their word order, and there is a strong correlation between transfer performance and word embedding alignment between languages (e.g., ρs=0.94 on the task of NLI). Our results call for focus in multilingual models on explicitly improving word embedding alignment between languages rather than relying on its implicit emergence.",Anonymous,/forum?id=nWNq_AarEVd
5,cTFOIyRjowo,Unsupervised Compressive Text Summarisation with Reinforcement Learning,,,,,/pdf?id=cTFOIyRjowo,,,,,,,,,,,,,"Recently, compressive text summarisation offers a balance between the conciseness issue of extractive summarisation and the factual hallucination issue of abstractive summarisation. However, most existing compressive summarisation methods are supervised, relying on the expensive effort of creating a new training dataset with corresponding compressive summaries. In this paper, we propose an unsupervised compressive summarisation method that utilises reinforcement learning to optimise a summary's semantic coverage and fluency by simulating human judgment on summarisation quality. Our model consists of an extractor agent and a compressor agent, and both agents have a multi-head attentional pointer-based structure. The extractor agent first chooses salient sentences from a document, and then the compressor agent compresses these extracted sentences by selecting salient words to form a summary without using reference summaries to compute the summary reward. That is, a parallel dataset with document-summary pairs is not required to train the proposed model. To the best of our knowledge, our proposed method is the first work on unsupervised compressive summarisation. Experimental results on three widely used datasets, Newsroom, CNN/DM, and XSum, show that our model achieves promising performance and significant improvement on Newsroom in terms of the ROUGE metric.",Anonymous,/forum?id=cTFOIyRjowo
6,Qf2p8Zx4Ej,Iterative Decoding for Compositional Generalization in Transformers,,,,,/pdf?id=Qf2p8Zx4Ej,,,,,,,,,,,,,"Deep learning models generalize well to in-distribution data but struggle to generalize compositionally, i.e., to combine a set of learned primitives to solve more complex tasks. In sequence-to-sequence (seq2seq) learning, transformers are often unable to predict correct outputs for longer examples than those seen at training. This paper introduces iterative decoding, an alternative to seq2seq that (i) improves transformer compositional generalization in the PCFG and Cartesian product datasets and (ii) evidences that, in these datasets, seq2seq transformers do not learn iterations that are not unrolled. In iterative decoding, training examples are broken down into a sequence of intermediate steps that the transformer learns iteratively. At inference time, the intermediate outputs are fed back to the transformer as intermediate inputs until an end-of-iteration token is predicted. We conclude by illustrating some limitations of iterative decoding in the CFQ dataset.",Anonymous,/forum?id=Qf2p8Zx4Ej
7,uA7H-pojyxn,CPED: A Large-Scale Chinese Personalized and Emotional Dialogue Dataset for Open-Domain Conversation,,,,,/pdf?id=uA7H-pojyxn,,,,,,,,,,,,,"Recently, the personification and empathy capabilities of dialogue systems have received extensive attention from researchers. Although it is straightforward for humans to express themselves personally and empathically, this is highly difficult for dialogue systems since training data do not provide personalities or empathy knowledge. In this paper, we propose CPED, a large-scale Chinese personalized and emotional dialogue dataset, which consists of multisource knowledge related to empathy and personal characteristic. This knowledge covers 13 emotions, gender, Big Five personality traits, 19 dialogue acts and other knowledge. CPED contains more than 12K dialogues of 392 speakers from 40 TV shows. We also provide several strong baselines for open-domain conversation generation. The results show that explicitly infusing personalized knowledge and emotional information improves the personification level and empathy ability of dialogue systems, but the infusion method needs to be further studied. The dataset and baselines will be released on https://github.com/***/CPED.",Anonymous,/forum?id=uA7H-pojyxn
8,uH6GgEf9iq0,A Two-Stream AMR-enhanced Model for Document-level Event Argument Extraction,,,,,/pdf?id=uH6GgEf9iq0,,,,,,,,,,,,,"Most previous studies aim at extracting events from a single sentence, while document-level event extraction still remains under-explored. In this paper, we focus on extracting event arguments from an entire document, which mainly faces two critical problems: a) the long-distance dependency between trigger and arguments over sentences; b) the distracting context towards an event in the document. To address these issues, we propose a \textbf{T}wo-\textbf{S}tream \textbf{A}bstract meaning \textbf{R}epresentation enhanced extraction model (TSAR). TSAR encodes the document from different perspectives by a two-stream encoding module, to utilize local and global information and lower the impact of distracting context. Besides, TSAR introduces an AMR-guided interaction module to capture both intra-sentential and inter-sentential features, based on the locally and globally constructed AMR semantic graphs. An auxiliary boundary loss is introduced to enhance the boundary information for text spans explicitly. Extensive experiments illustrate that TSAR outperforms previous state-of-the-art by a large margin, with 2.54 F1 and 5.13 F1 performance gain on the public RAMS and WikiEvents datasets respectively, showing the superiority in the cross-sentence arguments extraction. We will release our code upon acceptance.",Anonymous,/forum?id=uH6GgEf9iq0
9,L3TcOq4D2cD,Is syntax structure modeling worth? Leveraging pattern-driven modeling to enable affordable sentiment dependency learning,,,,,/pdf?id=L3TcOq4D2cD,,,,,,,,,,,,,"Is structure information modeling really worth in Aspect-based sentiment classification (ABSC)? Recent popular works tend to exploit syntactic information guiding sentiment dependency parsing, i.e., structure-based sentiment dependency learning. However, many works fall into the trap that confusing the concepts between syntax dependency and sentiment dependency. Besides, structure information (e.g., syntactic dependency tree) usually consumes expensive computational resources due to the extraction of the adjacent matrix. Instead, we believe the sentiment dependency mostly occurs between adjacent aspects. By proposing the sentiment patterns (SP) to boost the sentiment dependency learning, we introduce the Local dependency aggregating (Lena) to explore sentiment dependency in the text. Experiments show that Lena is more efficient than existing structure-based models without dependency matrix constructing and modeling expense. The performance on all five public ABSC datasets makes a big step compared to state-of-the-art models, and our work could inspire future research focusing on efficient local sentient dependency modeling.",Anonymous,/forum?id=L3TcOq4D2cD
10,zejCew2cMr0,"Toward Preference-Aware Story Evaluation via Ranking, Rating and Reasoning",,,,,/pdf?id=zejCew2cMr0,,,,,,,,,,,,,"Existing automatic story evaluation methods place a premium on story coherence, deviating from human preference. We go beyond such restrictions by presenting a more challenging task of \textbf{preference-aware story evaluation}. Given either a machine-generated or a human-written story, the task requires the machine to output a preference score that corresponds to human preference, along with specific ratings and comments for various aspects (e.g., opening, character-shaping). To support this novel task, we introduce a new dataset, namely \textbf{StoR3}, comprising (i) 100k ranked story pairs; and (ii) a set of 46k ratings and comments on various aspects of the story. To move towards preference-aware evaluation, we propose a model using the \textit{upvote count} as the criterion. The experiments show that the scores obtained by our model have a high correlation to human preference. Additionally, we discovered that the combination of aspect ratings and comments improves performance. Our dataset and benchmarks are publicly available to advance the research of story evaluation tasks.",Anonymous,/forum?id=zejCew2cMr0
11,G2FmuKQv7nd,Incorporate Dependency Relation Knowledge into Transformer Block for Multi-turn Dialogue Generation,,,,,/pdf?id=G2FmuKQv7nd,,,,,,,,,,,,,"Because of the compositionality of natural language, syntactic structure is one of the key factors for semantic understanding. However, the Transformer block, which is widely used for obtaining the distributed representations of sentences in dialogue generation tasks, views sentences as a sequence of words and does not effectively learn the syntactic structure. In this work, we explore how to effectively incorporate dependency relation knowledge that contains syntactic structure information into Transformer block and propose Dependency Relation Attention(DRA). Experimental results demonstrate that DRA can further improve the performance of state-of-the-art models for multi-turn dialogue generation.",Anonymous,/forum?id=G2FmuKQv7nd
12,6Lf2H-Bx59N,Anticipation-free Training for Simultaneous Translation,,,,,/pdf?id=6Lf2H-Bx59N,,,,,,,,,,,,,"Simultaneous translation (SimulMT) speeds up the translation process by starting to translate before the source sentence is completely available. It is difficult due to limited context and word order difference between languages. Existing methods increase latency or introduce adaptive read-write policies for SimulMT models to handle local reordering and improve translation quality. However, the long-distance reordering would make the SimulMT models learn translation mistakenly. Specifically, the model may be forced to predict target tokens when the corresponding source tokens have not been read. This leads to aggressive anticipation during inference, resulting in the hallucination phenomenon. To mitigate this problem, we propose a new framework that decompose the translation process into the monotonic translation step and the reordering step, and we model the latter by the auxiliary sorting network (ASN). The ASN rearranges the hidden states to match the order in the target language, so that the SimulMT model could learn to translate more reasonably. The entire model is optimized end-to-end and does not rely on external aligners or data. During inference, ASN is removed to achieve streaming. Experiments show the proposed framework could outperform previous methods with less latency.",Anonymous,/forum?id=6Lf2H-Bx59N
13,GsqkBlCfE_7,Explaining Ranking Models using Multiple Explainers,,,,,/pdf?id=GsqkBlCfE_7,,,,,,,,,,,,,"Current approaches to interpreting complex ranking models are based on local approximations of the ranking model using a simple ranker in the locality of the query. Since rankings have multiple relevance factors and are aggregations of predictions, existing approaches that use a single ranker might not be sufficient to approximate a complex model resulting in low local fidelity. In this paper, we overcome this problem by considering multiple simple rankers for better approximating the black box ranking model. We pose the problem of local approximation as a Generalized Preference Coverage (GPC) problem that incorporates multiple simple rankers towards the post-hoc interpretability of ranking models. Our approach Multiplex uses a linear programming approach to judiciously extract the explanation terms. We conduct extensive experiments on a variety of ranking models and report fidelity improvements of 37%−54% over existing baselines and competitors. We finally qualitatively compare modern neural ranking models in terms of their explanations to better understand the differences between them, showcasing our explainers' practical utility.",Anonymous,/forum?id=GsqkBlCfE_7
14,U-37qbGWEN4,Compositional Generalization Requires Compositional Parsers,,,,,/pdf?id=U-37qbGWEN4,,,,,,,,,,,,,"A rapidly growing body of research on compositional generalization investigates the ability of a semantic parser to dynamically recombine linguistic elements seen in training into unseen sequences. We present a systematic comparison of sequence-to-sequence models and models guided by compositional principles on the recent COGS corpus (Kim and Linzen, 2020). Though seq2seq models can perform well on lexical tasks, they perform with near-zero accuracy on structural generalization tasks that require novel syntactic structures; this holds true even when they are trained to predict syntax instead of semantics. In contrast, compositional models achieve near-perfect accuracy on structural generalization; we present new results confirming this from the AM parser (Groschwitz et al., 2021). Our findings show structural generalization is a key measure of compositional generalization and requires models that are aware of complex structure.",Anonymous,/forum?id=U-37qbGWEN4
15,jZI8FwxeuDV,"MOVER: Mask, Over-generate and Rank for Hyperbole Generation",,,,,/pdf?id=jZI8FwxeuDV,,,,,,,,,,,,,"Despite being a common figure of speech, hyperbole is under-researched in Figurative Language Processing. In this paper, we tackle the challenging task of hyperbole generation to transfer a literal sentence into its hyperbolic paraphrase. To address the lack of available hyperbolic sentences, we construct HYPO-XL, the first large-scale hyperbole corpus containing 17,862 hyperbolic sentences in a non-trivial way. Based on our corpus, we propose an unsupervised method for hyperbole generation that does not require parallel literal-hyperbole pairs. During training, we fine-tune BART to infill masked hyperbolic spans of sentences from HYPO-XL. During inference, we mask part of an input literal sentence and over-generate multiple possible hyperbolic versions. Then a BERT-based ranker selects the best candidate by hyperbolicity and paraphrase quality. Automatic and human evaluation results show that our model is effective at generating hyperbolic paraphrase sentences and outperforms several baseline systems.",Anonymous,/forum?id=jZI8FwxeuDV
16,eHMpE26Z2LC,A Survey on Stance Detection for Mis- and Disinformation Identification,,,,,/pdf?id=eHMpE26Z2LC,,,,,,,,,,,,,"Understanding attitudes expressed in texts, also known as stance detection, plays an important role in systems for detecting false information online, be it misinformation (unintentionally false) or disinformation (intentionally false information). Stance detection has been framed in different ways, including (a) as a component of fact-checking, rumour detection, and detecting previously fact-checked claims, or (b) as a task in its own right. While there have been prior efforts to contrast stance detection with other related tasks such as argumentation mining and sentiment analysis, there is no existing survey on examining the relationship between stance detection and mis- and disinformation detection. Here, we aim to bridge this gap by reviewing and analysing existing work in this area, with mis- and disinformation in focus, and discussing lessons learnt and future challenges.",Anonymous,/forum?id=eHMpE26Z2LC
17,honYC2DZPnf,Joint Mitigation of Interactional Bias,,,,,/pdf?id=honYC2DZPnf,,,,,,,,,,,,,"Machine learning algorithms have been found discriminative against groups of different social identities, e.g., gender and race. With the detrimental effects of these algorithmic biases, researchers proposed promising approaches for bias mitigation, typically designed for individual bias types. Due to the complex nature of social bias, we argue it is important to study how different biases interact with each other, i.e., how mitigating one bias type (e.g., gender) influences the bias results regarding other social identities (e.g., race and religion). We further question whether jointly debiasing multiple types of bias is desired in different contexts, e.g., when correlations between biases are different. To address these research questions, we examine bias mitigation in two NLP tasks -- toxicity detection and word embeddings -- on three social identities, i.e., race, gender, and religion. Empirical findings based on benchmark datasets suggest that different biases can be correlated and therefore, warranting attention for future research on joint bias mitigation.",Anonymous,/forum?id=honYC2DZPnf
18,Dgt1yxotA-V,LPC: A Logits and Parameter Calibration Framework on Continual Learning,,,,,/pdf?id=Dgt1yxotA-V,,,,,,,,,,,,,"Deep learning based pre-trained natural language processing (NLP) models typically pre-train on large unlabeled corpora first, then fine-tune on new tasks. When we execute such a paradigm on continuously sequential tasks, the model will suffer from the catastrophic forgetting problem (i.e., they forget the parameters learned in previous tasks when we train the model on newly emerged tasks). Inspired by the idea of how humans learn things, we aim to maintain the old knowledge when we transfer to novel contents and calibrate the old and new knowledge. We propose a Logits and Parameter Calibration (LPC) framework to reduce the catastrophic forgetting in the continual learning process. The proposed framework includes two important components, the Logits Calibration (LC) and Parameter Calibration (PC). The core idea is to reduce the difference between old knowledge and new knowledge by doing calibration on logits and parameters so that the model can maintain old knowledge while learning new tasks without preserving data in previous tasks. First, we preserve the parameters learned from the base tasks. Second, we train the existing model on novel tasks and estimate the difference between base logits and parameters and novel logits and parameters. Third, we drift from the base tasks to novel tasks gradually. Furthermore, we integrate the logtis and parameter calibration into a brand-new optimization algorithm. Finally, we do experiments on 7 scenarios of the GLUE (the General Language Understanding Evaluation) benchmark. The experimental results show that our model achieves state-of-the-art performance on all 7 scenarios.",Anonymous,/forum?id=Dgt1yxotA-V
19,sOFobghoWr0,PaCo: Preconditions Attributed to Commonsense Knowledge,,,,,/pdf?id=sOFobghoWr0,,,,,,,,,,,,,"Humans can seamlessly reason with circumstantial preconditions of commonsense knowledge. We understand that ""a glass is used for drinking water"", unless ""the glass is broken"" or ""the water is toxic"". Despite state-of-the-art(SOTA) language models’ (LMs) impressive performance on inferring commonsense knowledge, it is unclear whether they understand the circumstantial preconditions. To address this gap, we propose a novel challenge of reasoning with circumstantial preconditions. We collect a dataset, called PaCo, consisting of 12.4 thousand preconditions of commonsense statements expressed in natural language. Based on this dataset, we create three canonical evaluation tasks and use them to examine the capability of existing LMs to understand situational preconditions. Our results reveal a 10-30% gap between machine and human performance on our tasks, which shows that reasoning with preconditions is an open challenge. Upon acceptance, we will release the dataset and the code used to test models.",Anonymous,/forum?id=sOFobghoWr0
20,xVRmm332G0z,Understanding Attention for Vision-and-Language Tasks,,,,,/pdf?id=xVRmm332G0z,,,,,,,,,,,,,"Attention mechanism has been used as an important component across Vision-and-Language(VL) tasks in order to bridge the semantic gap between visual and textual features. While attention has been widely used in VL tasks, it has not been examined the capability of different attention alignment calculation in bridging the semantic gap between visual and textual clues. In this research, we conduct a comprehensive analysis on understanding the role of attention alignment by looking into the attention score calculation methods and check how it actually represents the visual region's and textual token's significance for the global assessment. We also analyse the conditions which attention score calculation mechanism would be more (or less) interpretable, and which may impact the model performance on three different VL tasks, including visual question answering, text-to-image generation, text-and-image matching (both sentence and image retrieval). Our analysis is the first of its kind and provides useful insights of the importance of each attention alignment score calculation when applied at the training phase of VL tasks, commonly ignored in attention-based cross modal models, and/or pretrained models.",Anonymous,/forum?id=xVRmm332G0z
21,bwIOahh3kHO,RoViST: Learning Robust Metrics for Visual Storytelling,,,,,/pdf?id=bwIOahh3kHO,,,,,,,,,,,,,"Visual storytelling (VST) is the task of generating a story paragraph that describes a given image sequence. Most existing storytelling approaches have evaluated their models using traditional natural language generation metrics like BLEU or CIDEr. However, such metrics based on n-gram matching tend to have poor correlation with human evaluation scores and do not explicitly consider other criteria necessary for storytelling such as sentence structure or topic coherence. Moreover, a single score is not enough to assess a story as it does not inform us about what specific errors were made by the model. In this paper, we propose 3 evaluation metrics sets that analyses which aspects we would look for in a good story: 1) visual grounding, 2) coherence, and 3) non-redundancy. We measure the reliability of our metric sets by analysing its correlation with human judgement scores on a sample of machine stories obtained from 4 state-of-the-arts models trained on the Visual Storytelling Dataset (VIST). Our metric sets outperforms other metrics on human correlation, and could be served as a learning based evaluation metric set that is complementary to existing rule-based metrics.",Anonymous,/forum?id=bwIOahh3kHO
22,yE5leTuWgvx,Your Answer is Incorrect... Would you like to know why? Introducing a Bilingual Short Answer Feedback Dataset,,,,,/pdf?id=yE5leTuWgvx,,,,,,,,,,,,,"Handing in a paper or exercise and merely receiving a ""bad"" or ""incorrect"" as feedback is not very helpful when the goal is to improve. Unfortunately, this is currently the kind of feedback given by many Automatic Short Answer Grading (ASAG) systems. One of the reasons for this is a lack of content-focused elaborated feedback datasets. To encourage research on explainable and understandable feedback systems, we present the Short Answer Feedback dataset (SAF). Similar to other ASAG datasets, SAF contains learner responses and reference answers to German and English questions. However, instead of only assigning a label or score to the learners' answers, SAF also contains elaborated feedback explaining the given score. Thus, SAF enables supervised training of models that grade answers and explain where and why mistakes were made. This paper discusses the need for enhanced feedback models in real-world pedagogical scenarios, describes the dataset annotation process, gives a comprehensive analysis of SAF, and demonstrates how SAF challenges T5 Transformer models.",Anonymous,/forum?id=yE5leTuWgvx
23,8sK2XmrxWOp,CalBERT - Code-mixed Adaptive Language representations using BERT,,,,,/pdf?id=8sK2XmrxWOp,,,,,,,,,,,,,"A code-mixed language is a type of language that involves the combination of two or more language varieties in its script or speech. Code-mixed language has become increasingly prevalent in recent times, especially on social media. However, the exponential increase in the usage of code-mixed language, especially in a country like India which is linguistically diverse has led to various inconsistencies. Analysis of text is now becoming harder to tackle because the language present is not consistent and does not work with predefined existing models which are monolingual. We propose a novel approach to improve performance in Transformers by introducing an additional step called ""Siamese Pre-Training"", which allows pre-trained monolingual Transformers to adapt language representations for code-mixed languages with a few examples of code-mixed data. Our studies show that CalBERT is able to improve performance over existing pre-trained Transformer architectures on downstream tasks such as sentiment analysis. Multiple CalBERT architectures beat the state of the art F1-score on the Sentiment Analysis for Indian Languages (SAIL) dataset, with the highest possible improvement being 5.1 points. CalBERT also achieves state-of-the-art accuracy on the IndicGLUE Product Reviews dataset by beating the benchmark by 0.4 points.",Anonymous,/forum?id=8sK2XmrxWOp
24,D2mnbjkpk1-,Learning to Win Lottery Tickets in BERT Transfer via Task-agnostic Mask Training,,,,,/pdf?id=D2mnbjkpk1-,,,,,,,,,,,,,"Recent studies on the lottery ticket hypothesis (LTH) show that pre-trained language models (PLMs) like BERT contain matching subnetworks that have similar transfer learning performance as the original PLM. These subnetworks are found using magnitude-based pruning. In this paper, we find that the BERT subnetworks have even more potential than these studies have shown. Firstly, we discover that the success of magnitude pruning can be attributed to the preserved pre-training performance, which correlates with the downstream transferability. Inspired by this, we propose to directly optimize the subnetwork structure towards the pre-training objectives, which can better preserve the pre-training performance. Specifically, we train binary masks over model weights on the pre-training tasks, with the aim of preserving the universal transferability of the subnetwork, which is agnostic to any specific downstream tasks. We then fine-tune the subnetworks on the GLUE benchmark and the SQuAD dataset. The results show that, compared with magnitude pruning, mask training can effectively find BERT subnetworks with improved overall performance on downstream tasks. Moreover, our method is also more efficient in searching subnetworks and more advantageous when fine-tuning within a certain range of data scarcity. Our code will be released upon publication.",Anonymous,/forum?id=D2mnbjkpk1-
25,awj1UmAX6l1,Few-Shot Self-Rationalization with Natural Language Prompts,,,,,/pdf?id=awj1UmAX6l1,,,,,,,,,,,,,"Self-rationalization models that predict task labels and generate free-text elaborations for their predictions could enable more intuitive interaction with NLP systems. These models are, however, currently trained with a large amount of human-written free-text explanations for each task which hinders their broader usage. We propose to study a more realistic setting of self-rationalization using few training examples. We present FEB---a standardized collection of four existing English-language datasets and associated metrics. We identify the right prompting approach by extensively exploring natural language prompts on FEB. Then, by using this prompt and scaling the model size, we demonstrate that making progress on few-shot self-rationalization is possible. We show there is still ample room for improvement in this task: the average plausibility of generated explanations assessed by human annotators is at most 51\%, while plausibility of human explanations is 76\%. We hope that FEB and our proposed approach will spur the community to take on the few-shot self-rationalization challenge.",Anonymous,/forum?id=awj1UmAX6l1
26,h7FcdZf07sT,Improving Tokenisation by Alternative Treatment of Spaces,,,,,/pdf?id=h7FcdZf07sT,,,,,,,,,,,,,"Tokenisation is the first step in almost all NLP tasks, and state-of-the-art transformer-based language models all use subword tokenisation algorithms to process input text. Existing algorithms have problems, often producing tokenisations of limited linguistic validity, and representing equivalent strings differently depending on their position within a word. We hypothesise that these problems hinder the ability of transformer-based models to handle complex words, and suggest that these problems are a result of allowing tokens to include spaces. We thus experiment with an alternative tokenisation approach where spaces are always treated as individual tokens, finding it alleviates existing problems, improving performance of models. Concretely, we apply a modification to the BPE and Unigram algorithms which implements this approach, and find it gives more morphologically correct tokenisations, in particular when handling prefixes. In addition, we show that the modified algorithms give improved performance on downstream NLP tasks that involve handling complex words, whilst having no detrimental effect on performance in general natural language understanding tasks. Given the results of our experiments, we advocate for always treating spaces as individual tokens as a superior tokenisation method.",Anonymous,/forum?id=h7FcdZf07sT
27,qv6scbpp2Lb,"Adversarially Constructed Evaluation Sets Are More Challenging, but May Not Be Fair",,,,,/pdf?id=qv6scbpp2Lb,,,,,,,,,,,,,"More capable language models increasingly saturate existing task benchmarks, in some cases outperforming humans, leaving little headroom with which to measure further progress. Adversarial dataset creation has been proposed as a strategy to construct more challenging datasets, and two common approaches are: (1) filtering out easy examples and (2) model-in-the-loop data collection. In this work, we study the impact of applying each approach to create more challenging evaluation datasets. We adapt the AFLite algorithm to filter evaluation data, and run experiments against 18 different adversary models. We find that AFLite indeed selects more challenging examples, lowering the performance of evaluated models more as stronger adversary models are used. However, the resulting ranking of models can also be unstable and highly sensitive to the choice of adversary model used. Moreover, AFLite oversamples examples with low annotator agreement, meaning that model comparisons hinge on the most contentiously labeled examples. Smaller-scale experiments on the adversarially collected datasets ANLI and AdversarialQA show similar findings, broadly lowering performance with stronger adversaries while disproportionately affecting the adversary model.",Anonymous,/forum?id=qv6scbpp2Lb
28,yqu82n5FZ9r,SHAP-Based Explanation Methods: A Review for NLP Interpretability,,,,,/pdf?id=yqu82n5FZ9r,,,,,,,,,,,,,"Model explanations are crucial for the transparent, safe, and trustworthy deployment of machine learning models. The \emph{SHapley Additive exPlanations} (SHAP) framework is considered by many to be a gold standard for local explanations thanks to its solid theoretical background and general applicability. In the years following its publication, several variants appeared in the literature---presenting adaptations in the core assumptions and target applications. In this work, we review all relevant SHAP-based interpretability approaches available to date and provide instructive examples as well as recommendations regarding their applicability to NLP use cases.",Anonymous,/forum?id=yqu82n5FZ9r
29,xiLVs5bDt2X,AutoGraphex: Zero-shot Biomedical Definition Generation with Automatic Prompting,,,,,/pdf?id=xiLVs5bDt2X,,,,,,,,,,,,,"Describing terminologies with definition texts is an important step towards understanding the scientific literature, especially for domains with limited labeled terminologies. Previous works have sought to design supervised neural text generation models to solve the biomedical terminology generation task, but most of them failed to define never-before-seen terminologies in newly emerging research fields. Here, we tackle this challenge by introducing a zero-shot definition generation model based on prompting, a recent approach for eliciting knowledge from pre-trained language models, with automatically generated prompts. Furthermore, we enhanced the biomedical terminology dataset by adding descriptive texts to each biomedical subdiscipline, thus enabling zero-shot learning scenarios. Our model outperformed existing supervised baseline and the baseline pre-trained language model that employs manually crafted prompts by up to 52 and 6 BLEU score, respectively.",Anonymous,/forum?id=xiLVs5bDt2X
30,cA5yHxUxYyz,Flat and Nested Negation and Uncertainty Detection with PubMed BERT,,,,,/pdf?id=cA5yHxUxYyz,,,,,,,,,,,,,"Negation and uncertainty detection is an oft-studied challenge in biomedical NLP. Annotation style for the task has not been standardized and as such, the existing datasets not only vary in domain but require various algorithmic designs due to their structural differences. We present a new negation detection dataset in two versions from clinical publications. We further developed two BERT-based models to evaluate on each dataset version. Both models treat the task as a token-level multi-class classification task, one of which is capable of assigning more than one label per token in the case of recursive nesting. Our models achieve F1 scores of 76% and 72% on the development and test sets, respectively.",Anonymous,/forum?id=cA5yHxUxYyz
31,5X2EozKeTQP,Towards Faithful Personalized Response Selection in Retrieval Based Dialog Systems,,,,,/pdf?id=5X2EozKeTQP,,,,,,,,,,,,,"Personalized response selection systems are generally grounded on persona. However, the angle of emotion influencing response selection is not explored. Also, faithfulness to the conversation context of these systems plunges when a contradictory or an off-topic response is selected. This paper makes an attempt to address these issues by proposing a suite of fusion strategies that capture the interaction between persona, emotion, and entailment information of the utterances. A concept-flow encoder is designed which capture the relevant concept knowledge both in context and responses. Ablation studies were done on Persona-Chat dataset show that incorporating emotion, entailment improves the accuracy of response selection. We combine our fusion strategies and concept-flow encoding to train a BERT based model which outperforms the previous methods by margins larger than 1.9% on original personas and 1.7% on revised personas in terms of hits@1 (top-1 accuracy), achieving a new state-of-the-art performance on the Persona-Chat dataset.",Anonymous,/forum?id=5X2EozKeTQP
32,YKK4Er13bHq,Hate Speech and Counter Speech Detection: Context Does Matter,,,,,/pdf?id=YKK4Er13bHq,,,,,,,,,,,,,"Hate speech is plaguing the cyberspace along with user-generated content. Adding counter speech has become an effective way to combat hate speech online. Existing datasets and models target either (a) hate speech or (b) hate and counter speech but disregard the context. This paper investigates the role of context in the annotation and detection of online hate and counter speech, where context is defined as the preceding comment in a conversation thread. We created a context-aware dataset for a 3-way classification task on Reddit comments: hate speech, counter speech, or neutral. Our analyses indicate that context is critical to identify hate and counter speech: human judgments change for most comments depending on whether we show annotators the context. A linguistic analysis draws insights into the language people use to express hate and counter speech. Experimental results show that neural networks obtain significantly better results if context is taken into account. We also present qualitative error analyses shedding light into (a) when and why context is beneficial and (b) the remaining errors made by our best model when context is taken into account.",Anonymous,/forum?id=YKK4Er13bHq
33,guKWlEnC5vK,Time Waits for No One! Analysis and Challenges of Temporal Misalignment,,,,,/pdf?id=guKWlEnC5vK,,,,,,,,,,,,,"When an NLP model is trained on text data from one time period and tested or deployed on data from another, the resulting temporal misalignment can degrade end-task performance. In this work, we establish a suite of eight diverse tasks across different domains (social media, science papers, news, and reviews) and periods of time (spanning five years or more) to quantify the effects of temporal misalignment. Our study is focused on the ubiquitous setting where a pretrained model is optionally adapted through continued domain-specific pretraining, followed by task-specific finetuning. We establish a suite of tasks across multiple domains to study temporal misalignment in modern NLP systems. We find stronger effects of temporal misalignment on task performance than have been previously reported. We also find that, while temporal adaptation through continued pretraining can help, these gains are small compared to task-specific finetuning on data from the target time period. Our findings motivate continued research to improve temporal robustness of NLP models.",Anonymous,/forum?id=guKWlEnC5vK
34,JwdZ0SQnJmU,About Time: Do Transformers Learn Temporal Verbal Aspect?,,,,,/pdf?id=JwdZ0SQnJmU,,,,,,,,,,,,,"Aspect is a linguistic concept that describes how an action, event, or state of a verb phrase is situated in time. In this paper, we explore whether different transformer models are capable of identifying aspectual features. We focus on two specific aspectual features: telicity and duration. Telicity marks whether the verb's action or state has an endpoint or not (telic/atelic), and duration denotes whether a verb expresses an action (dynamic) or a state (stative). These features are integral to the interpretation of natural language, but also hard to annotate and identify with NLP methods. We perform experiments in English and French, and our results show that transformer models adequately capture information on telicity and duration in their vectors, even in their non-finetuned forms, but are somewhat biased with regard to verb tense and word order.",Anonymous,/forum?id=JwdZ0SQnJmU
35,fwriKA474EL,DEMix Layers: Disentangling Domains for Modular Language Modeling,,,,,/pdf?id=fwriKA474EL,,,,,,,,,,,,,"We introduce a new domain expert mixture (DEMix) layer that enables conditioning a language model (LM) on the domain of the input text. A DEMix layer includes a collection of expert feedforward networks, each specialized to a domain, that makes the LM modular: experts can be mixed, added, or removed after initial training. Extensive experiments with autoregressive transformer LMs (up to 1.3B parameters) show that DEMix layers reduce test-time perplexity (especially for out-of-domain data), increase training efficiency, and enable rapid adaptation. Mixing experts during inference, using a parameter-free weighted ensemble, enables better generalization to heterogeneous or unseen domains. We also show it is possible to add experts to adapt to new domains without forgetting older ones, and remove experts to restrict access to unwanted domains. Overall, these results demonstrate benefits of domain modularity in language models.",Anonymous,/forum?id=fwriKA474EL
36,AHi6penZSdX,Knowledge Distillation Improves Stability in Retranslation-based Simultaneous Translation,,,,,/pdf?id=AHi6penZSdX,,,,,,,,,,,,,"In simultaneous translation, the \emph{retranslation} approach has the advantage of requiring no modifications to the inference engine. However in order to reduce the undesirable instability (flicker) in the output, previous work has resorted to increasing the latency through masking, and introducing specialised inference, losing the simplicity of the approach. In this paper, we argue that the flicker is caused by both non-monotonicity of the training data, and by non-determinism of the resulting model. Both of these can be addressed using knowledge distillation. We evaluate our approach using simultaneously interpreted test sets for English-German and English-Czech and demonstrate that the distilled models have an improved flicker-latency tradeoff, with quality similar to the original.",Anonymous,/forum?id=AHi6penZSdX
37,atVnNvbWgce,Creation and evaluation of timelines for longitudinal user posts,,,,,/pdf?id=atVnNvbWgce,,,,,,,,,,,,,"There is increasing interest to work with user generated content in social media and especially textual posts over time. Currently there is no consistent way of segmenting user posts into timelines in a meaningful way that can improve the quality and cost of manual annotation. Here we propose a set of methods for segmenting longitudinal user posts into timelines that are likely to contain interesting moments of change in a user's behaviour based on the content they have shared online and their online activity. We also propose a framework for evaluating the timelines returned in terms of containing candidate moments of change in close proximity to manually annotated timelines dense in such moments of change. Finally, we present a discussion of the linguistic content of highly ranked timelines.",Anonymous,/forum?id=atVnNvbWgce
38,yHvwawfRePq,PInKS: Preconditioned Commonsense Inference with Weak Supervision,,,,,/pdf?id=yHvwawfRePq,,,,,,,,,,,,,"Reasoning with preconditions such as ""glass can be used for drinking water unless the glass is shattered"" remains an open problem for language models. The main challenge lies in the scarcity of preconditions data and the model's lack of support for such reasoning. We present PInKS, Preconditioned Commonsense Inference with Weak Supervision, an improved model for reasoning with preconditions through minimum supervision. We show, both empirically and theoretically, that PInKS improves the results across the benchmarks on reasoning with the preconditions of commonsense knowledge~(up to 0.4 macro-f1 scores). We further investigate the robustness of our method through PAC-Bayesian informativeness analysis, recall measures, and ablation study.",Anonymous,/forum?id=yHvwawfRePq
39,IHmaooh0Uz2,Detection and Mitigation of Political Bias in Natural Language Processing: A Survey,,,,,/pdf?id=IHmaooh0Uz2,,,,,,,,,,,,,"With the increasing importance of Natural Language Processing (NLP) tools, their implications on the propagation of societal biases become more and more relevant. In this context, the analysis of political bias in manually written and automatically generated text is a relatively understudied field. Political bias refers to the preference or prejudice towards one political ideology over another. To increase the discourse in this subject area, we analyse contemporary studies on detecting and mitigating political bias in this literature review. We further discuss benefits and potential drawbacks of the considered methods and look at the ethical considerations involved with political bias in NLP, before we give suggestions for future studies.",Anonymous,/forum?id=IHmaooh0Uz2
40,4KMy7qryo5p,Contrastive Learning for Fair Representations,,,,,/pdf?id=4KMy7qryo5p,,,,,,,,,,,,,"Trained classification models can unintentionally lead to biased representations and predictions, which can reinforce societal preconceptions and stereotypes. Existing debiasing methods for classification models, such as adversarial training, are often expensive to train and fragile to optimise. Here, we propose a method for mitigating bias in classifier training by incorporating contrastive learning, in which instances sharing the same class label are encouraged to have similar representations, while instances sharing a protected attribute are forced further apart. In such a way our method learns representations that capture the task label in focused regions, while ensuring the protected attribute has diverse spread, and thus has limited impact on prediction and thereby results in fairer models. Extensive experimental results on three tasks show that: our method achieves fairer representations larger bias reduction than competitive baselines; it does so without sacrificing main task performance; and it generalizes across modalities and binary- and multi-class classification tasks, being conceptually simple and agnostic to network architecture, and incurring minimal additional compute cost.",Anonymous,/forum?id=4KMy7qryo5p
41,-cjifzq0diz,Speeding Up Entmax,,,,,/pdf?id=-cjifzq0diz,,,,,,,,,,,,,"Softmax is the de facto standard for normalizing logits in modern neural networks for language processing. However, by producing a dense probability distribution each token in the vocabulary has a nonzero chance of being selected at each generation step, leading to a variety of reported problems in text generation. α-entmax of Peters et al. (2019) solves this problem, but is unfortunately slower than softmax. In this paper, we propose an alternative to α-entmax, which keeps its virtuous characteristics, but is as fast as optimized softmax and achieves on par or better performance in machine translation task.",Anonymous,/forum?id=-cjifzq0diz
42,-PLePMk6VER,Embedding Convolutions for Short Text Extreme Classification with Millions of Labels,,,,,/pdf?id=-PLePMk6VER,,,,,,,,,,,,,"In this paper, we propose a convolutional architecture InceptionXML which is light-weight, yet powerful, and robust to the inherent lack of word-order in short-text queries in search and recommendation tasks. We demonstrate the efficacy of applying convolutions by recasting the operation along the embedding dimension instead of the word dimension as done in conventional usage of CNNs for text classification. Towards scaling our model to problems with millions of labels, we also propose InceptionXML+ framework. This addresses the shortcomings of the dynamic hard-negative mining framework in the recently proposed LightXML by improving the alignment between the label-shortlister and extreme classifier. InceptionXML+ is not only smaller than state-of-the-art deep extreme classifier, Astec, in terms of model size but also significantly outperforms it on popular benchmark datasets. For reproducibility, the code is made available as part of this submission.",Anonymous,/forum?id=-PLePMk6VER
43,8uwyP3vl6t5,WARM: A Weakly (+Semi) Supervised Math Word Problem Solver,,,,,/pdf?id=8uwyP3vl6t5,,,,,,,,,,,,,"Solving math word problems (MWPs) is an important and challenging problem in natural language processing. Existing approaches to solve MWPs require full supervision in the form of intermediate equations. However, labeling every MWP with its corresponding equations is a time-consuming and expensive task. In order to address this challenge of equation annotation, we propose a weakly supervised model for solving MWPs by requiring only the final answer as supervision. We approach this problem by first learning to generate the equation using the problem description and the final answer, which we subsequently use to train a supervised MWP solver. We propose and compare various weakly supervised techniques to learn to generate equations directly from the problem description and answer. Through extensive experiments, we demonstrate that without using equations for supervision, our approach achieves accuracy gains of 4.5% and 32% over the state-of-the-art weakly supervised approach (Hong et al., 2021), on the standard Math23K (Wang et al., 2017) and AllArith (Roy and Roth, 2017) datasets respectively. Additionally, we curate and release new datasets of roughly 10k MWPs each in English and in Hindi (a low-resource language). These datasets are suitable for training weakly supervised models. We also present an extension of WARM to semi-supervised learning and present further improvements on results, along with insights.",Anonymous,/forum?id=8uwyP3vl6t5
44,g_mpzwfbluI,Negative Sample is Negative in Its Own Way: Tailoring Negative Sentences for Image-Text Retrieval,,,,,/pdf?id=g_mpzwfbluI,,,,,,,,,,,,,"Matching model is essential for Image-Text Retrieval framework. Existing research usually train the model with a triplet loss and explore various strategy to retrieve hard negative sentences in the dataset. We argue that current retrieval-based negative sample construction approach is limited in the scale of the dataset thus fail to identify negative sample of high difficulty for every image. We propose our TAiloring neGative Sentences with Discrimination and Correction (TAGS-DC) to generate synthetic sentences automatically as negative samples. TAGS-DC is composed of masking and refilling to generate synthetic negative sentences with higher difficulty. To keep the difficulty during training, we mutually improve the retrieval and generation through parameter sharing. To further utilize fine-grained semantic of mismatch in the negative sentence, we propose two auxiliary tasks, namely word discrimination and word correction to improve the training. In experiments, we verify the effectiveness of our model on MS-COCO and Flickr30K compared with current state-of-the-art models and demonstrates its robustness and faithfulness in the further analysis.",Anonymous,/forum?id=g_mpzwfbluI
45,utUL7W4FpGx,Balancing out Bias: Achieving Fairness Through Balanced Training,,,,,/pdf?id=utUL7W4FpGx,,,,,,,,,,,,,"Bias in natural language processing manifests as disparities in error rates across author demographics, typically disadvantaging minority groups. Although dataset balancing has been shown to be effective in mitigating bias, existing approaches do not directly account for correlations between author demographics and linguistic variables. To achieve Equal Opportunity fairness, this paper introduces a simple but highly effective objective for countering bias using balanced training. We extend the method in the form of a gated model, which incorporates protected attributes as input, and show that it is effective at reducing bias in predictions through demographic input perturbation, outperforming all other bias mitigation techniques when combined with balanced training.",Anonymous,/forum?id=utUL7W4FpGx
46,4xNbfr01sn2,BnPC: A Corpus for Paraphrase Detection in Bangla,,,,,/pdf?id=4xNbfr01sn2,,,,,,,,,,,,,"In this paper, we present the first benchmark dataset for paraphrase detection in Bangla language. Despite being the sixth most spoken language in the world, paraphrase identification in the Bangla language is barely explored. Our dataset contains 8,787 human-annotated sentence pairs collected from a total of 23 newspaper outlets' headlines on four categories. We explore different linguistic features and pre-trained language models to benchmark the dataset. We perform a human evaluation experiment to obtain a better understanding of the task's constraints, which reveals intriguing insights. We make our dataset and code publicly available.",Anonymous,/forum?id=4xNbfr01sn2
47,gREbyGxZ8c0,ImaginE: An Imagination-Based Automatic Evaluation Metric for Natural Language Generation,,,,,/pdf?id=gREbyGxZ8c0,,,,,,,,,,,,,"Automatic evaluations for natural language generation conventionally rely on token-level or embedding-level comparisons with the text references. This is different from human evaluation manners, in which people also form pictures of the text contents in their minds during reading. In this work, we propose ImaginE, an imagination-based automatic evaluation metric for natural language generation. With the help of CLIP and DALL-E, two cross-modal models pre-trained on large-scale image-text pairs, we automatically generate an image as the embodied imagination for the text snippet, and compute the imagination similarity using contextual embeddings. Experiments spanning several text generation tasks demonstrate that adding imagination with our ImaginE displays great potential in introducing multi-modal information into NLG evaluation, and improves existing automatic metrics’ correlations with human similarity judgments in many circumstances.",Anonymous,/forum?id=gREbyGxZ8c0
48,qitQJKkhEZj,A Survey on Multimodal Disinformation Detection,,,,,/pdf?id=qitQJKkhEZj,,,,,,,,,,,,,"Recent years have witnessed the proliferation of offensive content online such as fake news, propaganda, misinformation, and disinformation. While initially this was mostly about textual content, over time images and videos gained popularity, as they are much easier to consume, attract more attention, and spread further than simple text. As a result, researchers started leveraging different modalities and combinations thereof to combat online multimodal offensive content. In this study, we offer a survey that carefully studies the state-of-the-art on multimodal disinformation detection covering various combinations of modalities: text, images, speech, video, social media network structure, and temporal information. Moreover, while some studies focused on factuality, others investigated how harmful the content is. While these two components in the definition of disinformation -- (i) factuality, and (ii) harmfulness, are equally important, they are typically studied in isolation. Thus, we argue for the need to tackle disinformation detection by taking into account multiple modalities as well as both factuality and harmfulness, in the same framework. Finally, we discuss current challenges and future research directions.",Anonymous,/forum?id=qitQJKkhEZj
49,wcTzgV4y9gU,Literature-Augmented Clinical Outcome Prediction,,,,,/pdf?id=wcTzgV4y9gU,,,,,,,,,,,,,"We present BEEP (Biomedical Evidence-Enhanced Predictions), a novel approach for clinical outcome prediction that retrieves patient-specific medical literature and incorporates it into predictive models. Based on each individual patient's clinical notes, we train language models (LMs) to find relevant papers and fuse them with information from notes to predict outcomes such as in-hospital mortality. We develop methods to retrieve literature based on noisy, information-dense patient notes, and to augment existing outcome prediction models with retrieved papers in a manner that maximizes predictive accuracy. Our approach boosts predictive performance on three important clinical tasks in comparison to strong recent LM baselines, increasing F1 by up to 5 points and precision@Top-K by a large margin of over 25%.",Anonymous,/forum?id=wcTzgV4y9gU
50,yoZhDj8j8dK,Multi-Vector Models with Textual Guidance for Fine-Grained Scientific Document Similarity,,,,,/pdf?id=yoZhDj8j8dK,,,,,,,,,,,,,"We present a new scientific document similarity model based on matching fine-grained aspects of texts. To train our model, we exploit a naturally-occurring source of supervision: sentences in the full-text of papers that cite multiple papers together (co-citations). Such co-citations not only reflect close paper relatedness, but also provide textual descriptions of how the co-cited papers are related. This novel form of textual supervision is used for learning to match aspects across papers. We develop multi-vector representations where vectors correspond to sentence-level aspects of documents, and present two methods for aspect matching: (1) A fast method that only matches single aspects, and (2) a method that makes sparse multiple matches with an Optimal Transport mechanism that computes an Earth Mover's Distance between aspects. Our approach improves performance on document similarity tasks in four datasets. Further, our fast single-match method achieves competitive results, paving the way for applying fine-grained similarity to large scientific corpora.",Anonymous,/forum?id=yoZhDj8j8dK
51,QtioS583qVQ,“Find Me a Dataset”: Scientific Dataset Recommendation from Method Descriptions,,,,,/pdf?id=QtioS583qVQ,,,,,,,,,,,,,"Much of modern science relies on public datasets to develop research ideas. Finding a dataset for a given task can be difficult, particularly for new researchers. We aim to improve the process of dataset discovery by introducing a system called DatasetFinder which recommends relevant datasets given a short natural language description of a research idea. For the new task of dataset recommendation, we construct an English-language dataset that leverages existing annotations and compare several ranking models on this dataset. We also compare our proposed models against existing commercial search engines and find evidence that leveraging natural language descriptions improves search relevance. To encourage development on this new task, we release our constructed dataset and models to the public.",Anonymous,/forum?id=QtioS583qVQ
52,pY5QUHPRhNN,ME-GCN: Multi-dimensional Edge-Enhanced Graph Convolutional Networks for Semi-supervised Text Classification,,,,,/pdf?id=pY5QUHPRhNN,,,,,,,,,,,,,"Compared to sequential learning models, graph-based neural networks exhibit excellent ability in capturing global information and have been used for semi-supervised learning tasks, including citation network analysis or text classification. Most Graph Convolutional Networks are designed with the single-dimensional edge feature and failed to utilise the rich edge information about graphs. In this paper, we introduce the ME-GCN (Multi-dimensional Edge-enhanced Graph Convolutional Networks) for semi-supervised text classification. A text graph for an entire corpus is firstly constructed to describe the undirected and multi-dimensional relationship of word-to-word, document-document, and word-to-document. The graph is initialised with corpus-trained multi-dimensional word and document node representation, and the relations are represented according to the distance of those words/documents nodes. Then, the generated graph is trained with ME-GCN, which considers the edge features as multi-stream signals, and each stream performs a separate graph convolutional operation. Our ME-GCN can integrate a rich source of graph edge information of the entire text corpus. The results have demonstrated that our proposed model has significantly outperformed the state-of-the-art methods across eight benchmark datasets.",Anonymous,/forum?id=pY5QUHPRhNN
53,2YuWhczEQpM,HyEnA: A Hybrid Method for Extracting Arguments from Opinions,,,,,/pdf?id=2YuWhczEQpM,,,,,,,,,,,,,"The key arguments underlying a large and noisy set of opinions help understand the opinions quickly and accurately. Fully automated methods can extract arguments but (1) require large labeled datasets and (2) work well for known viewpoints, but not for novel points of view. We propose HyEnA, a hybrid (human + AI) method for extracting arguments from opinionated texts, combining the speed of automated processing with the understanding and reasoning capabilities of humans. We evaluate HyEnA on three feedback corpora on COVID-19 relaxation measures. We find that, on the one hand, HyEnA achieves higher coverage and precision than a state-of-the-art automated method, when compared on a common set of diverse opinions, justifying the need for human insight. On the other hand, HyEnA requires less human effort and does not compromise quality compared to (fully manual) expert analysis, demonstrating the benefit of combining human and machine intelligence.",Anonymous,/forum?id=2YuWhczEQpM
54,VmhFupFpnHK,D2U: Distance-to-Uniform Learning for Out-of-Scope Detection,,,,,/pdf?id=VmhFupFpnHK,,,,,,,,,,,,,"Supervised training with cross-entropy loss implicitly forces models to produce probability distributions that follow a discrete delta distribution. Model predictions in test time are expected to be similar to delta distributions if the classifier determines the class of an input correctly. However, the shape of the predicted probability distribution can become similar to the uniform distribution when the model cannot infer properly. We exploit this observation for detecting out-of-scope (OOS) utterances in conversational systems. Specifically, we propose a zero-shot post-processing step, called Distance-to-Uniform (D2U), exploiting not only the classification confidence score, but the shape of the entire output distribution. We later combine it with a learning procedure that uses D2U for loss calculation in the supervised setup. We conduct experiments using six publicly available datasets. Experimental results show that the performance of OOS detection is improved with our post-processing when there is no OOS training data, as well as with D2U learning procedure when OOS training data is available.",Anonymous,/forum?id=VmhFupFpnHK
55,5pkldg17bKu,Representation of ambiguity in pretrained models and the problem of domain specificity,,,,,/pdf?id=5pkldg17bKu,,,,,,,,,,,,,Recent developments in pretrained language models have led to many advances in NLP. These models have excelled at learning powerful contextual representations from very large corpora. Fine-tuning these models for downstream tasks has been one of the most used (and successful) approaches to solving a plethora of NLP problems. But how capable are these models in capturing subtle linguistic traits like ambiguity in their representations? We present results from a probing task designed to test the capability of the models to identify ambiguous sentences under different experimental settings. The results show how different pretrained models fare against each other in the same task. We also explore how domain specificity limits the representational capabilities of the probes.,Anonymous,/forum?id=5pkldg17bKu
56,ByVyEk50138,Enhancing Cross-lingual Prompting with Two-level Augmentation,,,,,/pdf?id=ByVyEk50138,,,,,,,,,,,,,"Prompting approaches show promising results in few-shot scenarios. However, its strength for multilingual/cross-lingual problems has not been fully exploited. Zhao and Schütze (2021) made initial explorations in this direction by presenting that cross-lingual prompting outperforms cross-lingual finetuning. In this paper, we first conduct sensitivity analysis on the effect of each component in cross-lingual prompting and derive Universal Prompting across languages. Based on this, we propose a two-level augmentation framework to further improve the performance of prompt-based cross-lingual transfer. Notably, for XNLI, our method achieves 46.54% with only 16 English training examples per class, significantly better than 34.99% of finetuning.",Anonymous,/forum?id=ByVyEk50138
57,zmxg59rhm3D,Cross-modal Contrastive Learning for Speech Translation,,,,,/pdf?id=zmxg59rhm3D,,,,,,,,,,,,,"How to learn similar representations for spoken utterances and their written text? We believe a unified and aligned representation of speech and text will lead to improvement in speech translation. To this end, we propose ConST, a cross-modal contrastive learning method for end-to-end speech-to-text translation. We evaluate ConST and a variety of previous baselines on multiple language directions (En-De/Fr/Ru) of a popular benchmark MuST-C. Experiments show that the proposed ConST consistently outperforms all previous methods, and achieves the state-of-the-art average BLEU of 28.5. The analysis further verifies that ConST indeed closes the representation gap of different modalities --- its learned representation improves the accuracy of cross-modal text retrieval from 4% to 88%.",Anonymous,/forum?id=zmxg59rhm3D
58,HuUbb1kRPCj,Analyzing Pretrained Transformers for Spatial Commonsense Capabilities,,,,,/pdf?id=HuUbb1kRPCj,,,,,,,,,,,,,"This paper investigates whether pretrained transformers can make simple decisions requiring spatial commonsense. More specifically, we investigate whether they can distinguish between likely and unlikely rooms where everyday objects may be found. We present a new benchmark with 10K (object, room) pairs and experiment with state-of-the-art transformers. Our results show that off-the-shelf pretrained transformers have no spatial commonsense capabilities beyond the simplest baselines but can learn from examples. More importantly, we show that distant supervision is an effective strategy to incorporate spatial commonsense into transformers, especially when evaluating with unseen rooms in training.",Anonymous,/forum?id=HuUbb1kRPCj
59,no-1TXR104a,HiURE: Hierarchical Exemplar Contrastive Learning for Unsupervised Relation Extraction,,,,,/pdf?id=no-1TXR104a,,,,,,,,,,,,,"Unsupervised relation extraction aims to extract the relationship between entities from natural language sentences without prior information on relational scope or distribution. Existing works either utilize self-supervised schemes to refine relational feature signals by iteratively leveraging adaptive clustering and classification that provoke gradual drift problems, or adopt instance-wise contrastive learning which unreasonably pushes apart those sentence pairs that are semantically similar. To overcome these defects, we propose a novel contrastive learning framework named HiURE, which has the capability to derive hierarchical signals from relational feature space using cross hierarchy attention and effectively optimize relation representation of sentences under exemplar-wise contrastive learning. Experimental results on two public datasets demonstrate the advanced effectiveness and robustness of HiURE on unsupervised relation extraction when compared with state-of-the-art models.",Anonymous,/forum?id=no-1TXR104a
60,Zw3hAh1aENp,Training Dynamics for Curriculum Learning: A Study on Monolingual and Cross-lingual NLU,,,,,/pdf?id=Zw3hAh1aENp,,,,,,,,,,,,,"Curriculum Learning (CL) is a technique of training models via ranking examples in a typically increasing difficulty trend with the aim of accelerating convergence and improving generalisability. However, current approaches for Natural Language Understanding (NLU) tasks use CL to improve in-domain model performance often via metrics that are detached from the model one aims to improve. In this work, instead, we employ CL for NLU by taking advantage of training dynamics as difficulty metrics, i.e. statistics that measure the behavior of the model at hand on data instances during training. In addition, we propose two modifications of existing CL schedulers based on these statistics. Differently from existing works, we focus on evaluating models on out-of-distribution data as well as languages other than English via zero-shot cross-lingual transfer. We show across four XNLU tasks that CL with training dynamics in both monolingual and cross-lingual settings can achieve significant speedups up to 58%. We also find that performance can be improved on challenging tasks, with OOD generalisation up by 8\% and zero-shot cross-lingual transfer up by 1%. Overall, experiments indicate that training dynamics can lead to better performing models and smoother training compared to other difficulty metrics.",Anonymous,/forum?id=Zw3hAh1aENp
61,OA-sluxOzcY,Pretrained Language Models Are All You Need For Text-to-SQL Schema Linking,,,,,/pdf?id=OA-sluxOzcY,,,,,,,,,,,,,"The use of Exact Match based Schema Linking (EMSL) has become standard in text-to-SQL: many state-of-the-art text-to-SQL models employ EMSL, and their performance drops significantly when the EMSL component is removed. In this work, however, we demonstrate that EMSL reduces robustness, rendering models vulnerable to synonym substitution and typos. Instead of relying on EMSL to make up for deficiencies in question-schema encoding, we show that by utilizing the pre-trained language model as the encoder, we can improve the performance without using EMSL, and thus the model is more robust. Our experiments suggest that EMSL is not the icing on the cake, but it is the one that introduces the vulnerability, and it can be replaced by better input encoding.",Anonymous,/forum?id=OA-sluxOzcY
62,W7Po4NhFicq,A High-Precision Health-relatedness Score for Phrases to Mine Cause-Effect Statements from the Web,,,,,/pdf?id=W7Po4NhFicq,,,,,,,,,,,,,"The measurement of the health-relatedness of a phrase is important when mining the web at scale for health information, e.g., when building a search engine or when carrying out health-sociological analyses. We propose a new termhood scoring scheme that allows for the prediction of the health-relatedness of phrases at high precision. An evaluation on several corpora of cause--effect statements (heuristically and professionally labeled) yields about 60\%~recall at over 90\%~precision, outperforming state-of-the-art vocabulary-based approaches and performing on par with BERT while being less resource-demanding. A new resource of over 4~million health-related cause--effect statements is compiled, such as ``Studies show that stress induces insomnia.'', which explicitly connect symptoms (`stress') as claimed causes for conditions (`insomnia'). It consists of over 4~million sentences from more than 2~million unique web pages and 234,000 unique websites.",Anonymous,/forum?id=W7Po4NhFicq
63,dK5m9SY90_W,Non-Autoregressive Machine Translation: It's Not as Fast as it Seems,,,,,/pdf?id=dK5m9SY90_W,,,,,,,,,,,,,"Efficient machine translation models are commercially important as they can increase inference speeds, and reduce costs and carbon emissions. Recently, there has been much interest in non-autoregressive (NAR) models, which promise faster translation. In parallel to the research on NAR models, there have been successful attempts to create optimized autoregressive models as part of the WMT shared task on efficient translation. In this paper, we point out flaws in the evaluation methodology present in the literature on NAR models and we provide fair comparison between a state-of-the-art NAR model and the autoregressive submissions to the shared task. We make the case for consistent evaluation of NAR models, and also for the importance of comparing NAR models with other widely used efficiency approaches. We run experiments with a connectionist-temporal-classification-based (CTC) NAR model implemented in C++ and compare it with AR models using wall clock times. Our results show that, although NAR models are faster on GPUs, with small batch sizes, they are nearly always slower under more realistic usage conditions. We call for more realistic and extensive evaluation of NAR models in future work.",Anonymous,/forum?id=dK5m9SY90_W
64,sYkhFm2kkzj,Challenging America: Modeling language in longer time scales,,,,,/pdf?id=sYkhFm2kkzj,,,,,,,,,,,,,"The aim of the paper is to apply, for historical texts, the methodology used commonly to solve various NLP tasks defined for contemporary data, i.e. pre-train and fine-tune large Transformer models. This paper introduces an ML challenge, named Challenging America (ChallAm), based on OCR-ed excerpts from historical newspapers collected from the Chronicling America portal. ChallAm provides a dataset of clippings, labeled with metadata on their origin, and paired with their textual contents retrieved by an OCR tool. Three, publicly available, ML tasks are defined in the challenge: to determine the article date, to detect the location of the issue, and to deduce a word in a text gap (cloze test). Strong baselines are provided for all three ChallAm tasks. In particular, we pre-trained a RoBERTa model from scratch from the historical texts. We also discuss the issues of discrimination and hate-speech present in the historical American texts.",Anonymous,/forum?id=sYkhFm2kkzj
65,zPFbyqOX8Cx,Cloze Evaluation for Deeper Understanding of Commonsense Stories in Indonesian,,,,,/pdf?id=zPFbyqOX8Cx,,,,,,,,,,,,,"Story comprehension that involves complex causal and temporal relations is imperative in NLP, but previous studies have focused on English, leaving open the question of how the findings generalize to other languages, such as Indonesian. In this paper, we follow the Story Cloze Test framework of Mostafazadeh et al. (2016) in evaluating story understanding in Indonesian, by constructing a four-sentence story with one correct ending and one incorrect ending. To investigate commonsense knowledge acquisition in language models, we experimented with: (1) a classification task to predict the correct ending; and (2) a generation task to complete the story with a single sentence. We investigate these tasks in two settings: (i) monolingual training and (ii) zero-shot cross-lingual transfer between Indonesian and English.",Anonymous,/forum?id=zPFbyqOX8Cx
66,RfVMLYiA7Ed,TASA: Twin Answer Sentences Attack for Adversarial Context Generation in Question Answering,,,,,/pdf?id=RfVMLYiA7Ed,,,,,,,,,,,,,"We present Twin Answer Sentences Attack (TASA), a novel question answering (QA) adversarial attack method that produces fluent and grammatical adversarial contexts while maintaining its gold answers. Despite phenomenal progresses on general adversarial attacks, few works have investigated the vulnerability and adversarial attack specifically for QA. In this work, we first investigate the biases in the existing models and discover that they heavily rely on keyword matching and ignore the relevant entities from the question. TASA explores the two biases above and attacks the target model in two folds: (1) lowering the model's confidence on the gold answer with a perturbed answer sentence; (2) misguiding the model towards a wrong answer with a distracting answer sentence. Equipped with designed beam search and filtering methods, TASA is able to attack the target model efficiently while sustaining the quality of contexts. Extensive experiments on four QA datasets and human evaluations demonstrate that TASA generates substantial-high-quality attacks than existing textual adversarial attack methods.",Anonymous,/forum?id=RfVMLYiA7Ed
67,4OFAG-W2Mi6,Diagnosing Vision-and-Language Navigation: What Really Matters,,,,,/pdf?id=4OFAG-W2Mi6,,,,,,,,,,,,,"Vision-and-language navigation (VLN) is a multimodal task where an agent follows natural language instructions and navigates in visual environments. Multiple setups have been proposed, and researchers apply new model architectures or training techniques to boost navigation performance. However, there still exist non-negligible gaps between machines' performance and human benchmarks. Moreover, the agents' inner mechanisms for navigation decisions remain unclear. To the best of our knowledge, how the agents perceive the multimodal input is under-studied and needs investigation. In this work, we conduct a series of diagnostic experiments to unveil agents' focus during navigation. Results show that indoor navigation agents refer to both object and direction tokens when making decisions. In contrast, outdoor navigation agents heavily rely on direction tokens and poorly understand the object tokens. The differences in dataset designs and the visual features lead to distinct behaviors on visual environment understanding. Many models claim that they can align object tokens with specific visual targets when it comes to vision-and-language alignments. We find unbalanced attention on the vision and text input and doubt the reliability of such cross-modal alignments.",Anonymous,/forum?id=4OFAG-W2Mi6
